import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Azure

The Mistral AI open and commercial models are available on the Microsoft Azure cloud platform. More specifically:

- Mistral Large can be deployed with a pay-as-you-go token based billing as a fully-managed service.
- Existing open-weight models can be deployed to pre-configured dedicated infrastructure on your Azure subscription.


## Deploying Mistral Large 

The Azure portal provides extensive documentation on how to deploy Mistral Large with:

- [Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral#reference-for-mistral-large-deployed-as-a-service)
- [Azure Machine Learning Studio](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-mistral)

## Querying the model

Once your model is deployed and provided that you have the relevant permissions, you should
be able to consume it.

<Tabs>
    <TabItem value="curl" label="curl" default>
        - `$AZURE_AI_MISTRAL_LARGE_ENDPOINT` is your endpoint URL, should be of the form `https://your-endpoint.inference.ai.azure.com/v1/chat/completions`.
        - `$AZURE_AI_MISTRAL_LARGE_KEY` is your authentication key.

        ```shell
        curl -s $AZURE_AI_MISTRAL_LARGE_ENDPOINT/v1/chat/completions \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $AZURE_AI_MISTRAL_LARGE_KEY" \
        -d '{
          "model": "azureai",
          "messages": [
            {
              "role": "user",
              "content": "Who is the most renowned French painter?"
            }
          ]
        }'
        ```
    </TabItem>
    <TabItem value="python" label="Python">

    You will need to define the following environment variables:
    - `AZURE_AI_MISTRAL_LARGE_ENDPOINT` is your endpoint URL, should be of the form `https://your-endpoint.inference.ai.azure.com/v1/chat/completions`.
    - `AZURE_AI_MISTRAL_LARGE_KEY` is your authentication key.

    You will also need to install the Mistral AI Python client, by following the instructions from [the repository](https://github.com/mistralai/client-python).

    Python Client

    ```shell
    pip install

    ```python
    import os
    from mistralai.client import MistralClient
    from mistralai.models.chat_completion import ChatMessage

    endpoint = os.environ["AZURE_AI_MISTRAL_LARGE_ENDPOINT"]
    api_key = os.environ["AZURE_AI_MISTRAL_LARGE_KEY"]
    model = "azureai"

    client = MistralClient(api_key=api_key,
                           endpoint=endpoint)

    # With streaming 
    for chunk in client.chat_stream(
        model=model,
        messages=[ChatMessage(role="user", content="What is the best French cheese?")],
    ):
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    ```
        
    </TabItem>
</Tabs>



## Going further

For other usage examples, you can also check the following notebooks:

- [Basic CLI with `curl` and Python web request](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/webrequests.ipynb)
- [Mistral AI Python client example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/mistralai.ipynb)
- [Langchain example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/langchain.ipynb)
- [LiteLLM example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/litellm.ipynb)
- [OpenAI SDK example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/openaisdk.ipynb)
