import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Azure

The Mistral AI open and commercial models are available on the Microsoft Azure cloud platform. More specifically:

- Mistral Large can be deployed with a pay-as-you-go token based billing as a fully-managed service.
- Existing open-weight models can be deployed to dedicated VMs on your Azure subscription.


## Deploying Mistral Large 

The Azure portal provides extensive documentation on how to deploy Mistral Large with:

- [Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral#reference-for-mistral-large-deployed-as-a-service)
- [Azure Machine Learning Studio](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-mistral)

## Querying the model

Once your model is deployed and provided that you have the relevant permissions, you should
be able to consume it.

<Tabs>
    <TabItem value="curl" label="curl" default>
    - `$AZURE_ENDPOINT_URL` is your endpoint URL, should be of the form `https://your-endpoint.inference.ai.azure.com/v1/chat/completions`.
    - `$AZURE_SECRET_KEY` is your authentication key.

    ```shell
    curl -X POST \
        --location $AZURE_ENDPOINT_URL/v1/chat/completions \
        --header 'Content-Type: application/json' \
        --header 'Authorization: $AZURE_SECRET_KEY' \
        --data '{\"messages\":[{\"content\":\"You are a helpful assistant.\",\"role\":\"system\"},{\"content\":\"What is good about Wuhan?\",\"role\":\"user\"}], \"max_tokens\": 50}'
    ```
    </TabItem>
    <TabItem value="python" label="Python">

    You will need to define the following environment variables:
    - `AZURE_ENDPOINT_URL` is your endpoint URL, should be of the form `https://your-endpoint.inference.ai.azure.com/v1/chat/completions`.
    - `AZURE_SECRET_KEY` is your authentication key.

    ```python
    from mistralai.client import MistralClient
    from mistralai.models.chat_completion import ChatMessage

    api_key = os.environ["AZURE_SECRET_KEY"]
    endpoint = os.environ["AZURE_ENDPOINT_URL"]
    model = "azureai"

    client = MistralClient(api_key=api_key,
                           endpoint=endpoint)
    messages = [
        ChatMessage(role="user", content="What is the best French cheese?")
    ]

    # With streaming
    stream_response = client.chat_stream(model=model, messages=messages)

    for chunk in stream_response:
        print(chunk.choices[0].delta.content)
    ```
        
    </TabItem>
</Tabs>



## Going further

For more extensive usage examples, you can also check the following notebooks:

- [Basic CLI with `curl` and Python web request](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/webrequests.ipynb)
- [Mistral AI Python client example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/mistralai.ipynb)
- [Langchain example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/langchain.ipynb)
- [LiteLLM example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/litellm.ipynb)
- [OpenAI SDK example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/openaisdk.ipynb)
