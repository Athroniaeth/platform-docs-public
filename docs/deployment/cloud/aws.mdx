---
id: aws
title: AWS Bedrock
sidebar_position: 3.22
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


You can deploy the following Mistral AI models on the AWS Bedrock service:
- Mistral 7B Instruct
- Mixtral 8x7B Instruct
- Mistral Small
- Mistral Large

This page provides a straightforward guide on how to get started on using 
Mistral Large as an AWS Bedrock foundational model.

## Pre-requisites

In order to query the model you will need:

- Access to an **AWS account** within a region that supports the AWS Bedrock service and 
  offers access to Mistral Large: see 
  [the AWS documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) 
  for model availability per region.
- An AWS **IAM principal** (user, role) with sufficient permissions, see
  [the AWS documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html)
  for more details.
- **Access to the Mistral AI models enabled** from the AWS Bedrock home page, see
  [the AWS documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)
  for more details.
- A local **code environment** set up with the relevant AWS SDK components, namely:
    - the AWS CLI: see [the AWS documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
      for the installation procedure.
    - the `boto3` Python library: see the [AWS documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html) 
      for the installation procedure.

## Querying the model

Before starting, make sure to properly configure the authentication credentials for your development
environment. 
[The AWS documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)
provides an in-depth explanation on the required steps.

<Tabs>
    <TabItem value="python" label="Python">

        ```python
        import boto3

        MISTRAL_LARGE_BEDROCK_ID = "mistral.mistral-large-2402-v1:0"
        AWS_REGION = "eu-west-3"

        bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=AWS_REGION)

        messages = [{"role": "user", "content": [{"text": "What is the best French cheese?"}]}]
        temperature = 0.0
        max_tokens = 1024

        params = {"modelId": MISTRAL_LARGE_BEDROCK_ID,
                  "messages": messages,
                  "inferenceConfig": {"temperature": temperature,
                                      "maxTokens": max_tokens}}

        resp = bedrock_client.converse(**params)

        print(resp["output"]["message"]["content"][0]["text"])
        ```
    </TabItem>
        <TabItem value="cli" label="CLI">
            ```shell
            aws bedrock-runtime invoke-model \
            --model-id "mistral.mistral-large-2402-v1:0" \
            --body '{"prompt": "What is the best French cheese?", "max_tokens": 512, "top_p": 0.8, "temperature": 0.5}' \
            resp.json \
            --cli-binary-format raw-in-base64-out 
            ```
    </TabItem>
</Tabs>

## Going further

You can find a more detailed user guide on the [AWS documentation on inference requests for Mistral models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html#model-parameters-mistral-request-response).

For more advanced examples, you can also check out the following notebooks:

- [Bedrock function calling with Mistral models](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/Bedrock_Mistral_function_calling.ipynb)
- [Advanced RAG pipeline for Mistral models with Q&A Automation and Model Evaluation using LlamaIndex and Ragas](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/Mistral_model_RAG_pipeline_evaluation.ipynb)
- [Transitioning from OpenAI to Mistral: a guide](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/Transition_from_openai_to_mistral.ipynb)
- [Abstract document summarization with Langchain using Mistral Large on Bedrock](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/Abstract%20Document%20Summarization%20with%20Langchain%20using%20Mistral%20Large%20on%20Bedrock.ipynb)
- [Advanced multi-chain routing with Langchain and Mistral models](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/Advanced_Multi-Chain_Routing_With_LangChain.ipynb)
- [Mistral Large prompting: getting started](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/mistral_large_getting_started_101.ipynb)
- [Getting started with Mistral Tool Use and the Converse API](https://github.com/aws-samples/bedrock-mistral-prompting-examples/blob/main/notebooks/Tool_Use_with_Mistral.ipynb)



